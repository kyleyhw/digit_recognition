\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\begin{document}

\section*{Neural Network Fundamentals}

\subsection*{Understanding $dZ$ in Backpropagation}

In the context of neural networks and backpropagation, $dZ$ represents the \textbf{gradient of the loss function with respect to the output of a layer \textit{before} the activation function is applied}. It is often referred to as the "pre-activation gradient."

Let's break this down:
\begin{itemize}
    \item \textbf{$Z$ (Pre-activation Output):} For a \texttt{Dense} layer, we calculated $Z = XW + b$. This $Z$ is the linear output of the layer before any non-linear activation function is applied.
    \item \textbf{Loss Function ($L$):} This function quantifies how well our neural network is performing. Our goal during training is to minimize this loss.
    \item \textbf{Gradient ($dL/dZ$):} This derivative tells us how much the loss $L$ changes with respect to a small change in $Z$. In backpropagation, we propagate gradients backward through the network. The \texttt{output_gradient} passed to a layer's \texttt{backward} method is precisely $dL/dZ$ from the subsequent layer. It represents the "error signal" that needs to be propagated further back to:
    \begin{enumerate}
        \item Update the weights ($W$) and biases ($b$) of the current layer.
        \item Compute the gradient for the input ($X$) of the current layer ($dX$), which is then passed to the preceding layer.
    \end{enumerate}
\end{itemize}
In essence, $dZ$ is the crucial piece of information that tells a layer how much its pre-activation output contributed to the overall error, enabling it to adjust its parameters and propagate the error signal backward through the network.

\subsection*{Activation Functions}

An \textbf{activation function} is a non-linear function applied to the output of a neuron or layer (specifically, to $Z$, the pre-activation output). Its primary purpose is to introduce non-linearity into the neural network.

\subsubsection*{Why Non-linearity?}
\begin{itemize}
    \item \textbf{Linearity Limitation:} Without activation functions, a neural network, no matter how many layers it has, would only be able to learn linear transformations. This means it could only model linear relationships between inputs and outputs. For example, a network of only \texttt{Dense} layers would simply be equivalent to a single \texttt{Dense} layer, as a composition of linear functions is still a linear function.
    \item \textbf{Modeling Complex Relationships:} Most real-world data is inherently non-linear. Activation functions allow neural networks to learn and approximate complex, non-linear functions, enabling them to model intricate patterns in data that linear models cannot capture.
\end{itemize}

\subsubsection*{Common Characteristics of Activation Functions:}
\begin{itemize}
    \item \textbf{Non-linearity:} Essential for learning complex patterns.
    \item \textbf{Differentiability:} Must be differentiable to allow for gradient-based optimization (backpropagation).
    \item \textbf{Monotonicity:} Often, but not always, monotonic (e.g., ReLU, Sigmoid).
    \item \textbf{Range:} Can be bounded (e.g., Sigmoid, Tanh) or unbounded (e.g., ReLU).
\end{itemize}

\subsubsection*{Examples of Activation Functions:}
\begin{itemize}
    \item \textbf{Sigmoid ($\sigma(x) = \frac{1}{1 + e^{-x}}$):} Squashes values between 0 and 1. Historically popular, but can suffer from vanishing gradients.
    \item \textbf{Tanh (Hyperbolic Tangent, $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$):} Squashes values between -1 and 1. Generally performs better than Sigmoid as its output is zero-centered.
    \item \textbf{ReLU (Rectified Linear Unit, $\text{ReLU}(x) = \max(0, x)$):} Outputs the input directly if positive, otherwise outputs zero. Very popular due to its computational efficiency and ability to mitigate vanishing gradients for positive inputs.
    \item \textbf{Softmax ($\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$):} Used in the output layer for multi-class classification. It converts raw scores (logits) into probabilities that sum to 1.
\end{itemize}
In summary, activation functions are the key components that give neural networks their power to learn and represent complex, non-linear relationships in data.

\hrule % Horizontal line separator

\section*{Mathematical Explanation of Gradients for Dense Layer}

Let's consider a single \texttt{Dense} layer in a neural network.
The forward pass of this layer performs a linear transformation.

\subsection*{Forward Pass}
Let:
\begin{itemize}
    \item $X \in \mathbb{R}^{B \times I}$ be the input matrix, where $B$ is the batch size and $I$ is the input size.
    \item $W \in \mathbb{R}^{I \times O}$ be the weight matrix, where $O$ is the output size.
    \item $b \in \mathbb{R}^{O}$ be the bias vector.
    \item $Z \in \mathbb{R}^{B \times O}$ be the output matrix before activation.
\end{itemize}
The forward pass is given by:
$$ Z = XW + b $$
(Here, $b$ is broadcasted across the batch dimension.)

\subsection*{Backward Pass}
We are given the gradient of the loss function $L$ with respect to the output of this layer, $Z$. Let this be $dZ \in \mathbb{R}^{B \times O}$, which is the \texttt{output_gradient} passed to the \texttt{backward} method. Our goal is to compute the gradients of $L$ with respect to the weights ($W$), biases ($b$), and the input ($X$).

\subsubsection*{1. Gradient with respect to Weights ($dW$)}
We want to find $\frac{\partial L}{\partial W}$. Using the chain rule, we can express this conceptually as $\frac{\partial L}{\partial W} = \frac{\partial L}{\partial Z} \cdot \frac{\partial Z}{\partial W}$.

Consider an individual element $Z_{ij}$ of the output matrix $Z$:
$$ Z_{ij} = \sum_{k=1}^{I} (X_{ik} W_{kj}) + b_j $$
To find $\frac{\partial L}{\partial W_{mn}}$ (the gradient with respect to a specific weight $W_{mn}$), we sum over all $i$ and $j$:
$$ \frac{\partial L}{\partial W_{mn}} = \sum_{i=1}^{B} \sum_{j=1}^{O} \left( \frac{\partial L}{\partial Z_{ij}} \cdot \frac{\partial Z_{ij}}{\partial W_{mn}} \right) $$
The term $\frac{\partial Z_{ij}}{\partial W_{mn}}$ is non-zero only when $j = n$ and $k = m$, in which case $\frac{\partial Z_{ij}}{\partial W_{mn}} = X_{im}$.
So, the expression simplifies to:
$$ \frac{\partial L}{\partial W_{mn}} = \sum_{i=1}^{B} \left( \frac{\partial L}{\partial Z_{in}} \cdot X_{im} \right) $$
In matrix form, this summation corresponds to the dot product of the transpose of the input $X$ and the gradient $dZ$:
$$ dW = X^T dZ $$
Where:
\begin{itemize}
    \item $X^T \in \mathbb{R}^{I \times B}$
    \item $dZ \in \mathbb{R}^{B \times O}$
    \item The resulting $dW \in \mathbb{R}^{I \times O}$, which matches the shape of $W$.
\end{itemize}

\subsubsection*{2. Gradient with respect to Biases ($db$)}
We want to find $\frac{\partial L}{\partial b}$.
For an individual element $Z_{ij}$:
$$ Z_{ij} = \sum_{k=1}^{I} (X_{ik} W_{kj}) + b_j $$
The term $\frac{\partial Z_{ij}}{\partial b_m}$ is non-zero only when $j = m$, in which case $\frac{\partial Z_{ij}}{\partial b_m} = 1$.
So, to find $\frac{\partial L}{\partial b_m}$:
$$ \frac{\partial L}{\partial b_m} = \sum_{i=1}^{B} \sum_{j=1}^{O} \left( \frac{\partial L}{\partial Z_{ij}} \cdot \frac{\partial Z_{ij}}{\partial b_m} \right) $$
$$ \frac{\partial L}{\partial b_m} = \sum_{i=1}^{B} \left( \frac{\partial L}{\partial Z_{im}} \cdot 1 \right) $$
This means that the gradient for each bias term $b_m$ is the sum of the corresponding $\frac{\partial L}{\partial Z_{im}}$ values across the batch dimension.
In matrix form, this is:
$$ db = \sum_{i=1}^{B} dZ_i $$
Which can be written using NumPy's summation along an axis:
$$ db = \text{np.sum}(dZ, \text{axis}=0) $$
Where:
\begin{itemize}
    \item $dZ \in \mathbb{R}^{B \times O}$
    \item The resulting $db \in \mathbb{R}^{O}$, which matches the shape of $b$.
\end{itemize}

\subsubsection*{3. Gradient with respect to Input ($dX$)}
We want to find $\frac{\partial L}{\partial X}$. This is the gradient that will be passed to the previous layer.
$$ \frac{\partial L}{\partial X_{mn}} = \sum_{i=1}^{B} \sum_{j=1}^{O} \left( \frac{\partial L}{\partial Z_{ij}} \cdot \frac{\partial Z_{ij}}{\partial X_{mn}} \right) $$
The term $\frac{\partial Z_{ij}}{\partial X_{mn}}$ is non-zero only when $i = m$ and $k = n$, in which case $\frac{\partial Z_{ij}}{\partial X_{mn}} = W_{nj}$.
So, the expression simplifies to:
$$ \frac{\partial L}{\partial X_{mn}} = \sum_{j=1}^{O} \left( \frac{\partial L}{\partial Z_{mj}} \cdot W_{nj} \right) $$
In matrix form, this summation corresponds to the dot product of $dZ$ and the transpose of the weights $W$:
$$ dX = dZ W^T $$
Where:
\begin{itemize}
    \item $dZ \in \mathbb{R}^{B \times O}$
    \item $W^T \in \mathbb{R}^{O \times I}$
    \item The resulting $dX \in \mathbb{R}^{B \times I}$, which matches the shape of $X$. This $dX$ is the \texttt{input_gradient} returned by the \texttt{backward} method.
\end{itemize}

This detailed derivation, based on the chain rule and matrix calculus, explains the use of dot products for efficient gradient computation across batches.

\end{document}
